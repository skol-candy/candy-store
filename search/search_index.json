{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Skol Candy","text":"<p>Skol Candy is an online general store specializing in treats and goodies.  The store has added an Event Automation backend that enables that help promote the agile development of features and modern operational practices.  As a customer, your experience includes being able login and order items for delivery.  You are also encouraged to leave product reviews.  As part of this lab, you will first explore this online shopping experience.  Next, you will work with the application's backend to enable new analytic and customer experience opportunities.</p> <p>The steps you will perform:</p> <ul> <li>Purchase from the Skol Candy e-Tail experience</li> <li>Explore events being produced into the backend</li> <li>Build a event processing flow that processes the events into your own topic</li> <li>Use an AI processing node to provide sentiment analysis against customer produced product reviews</li> <li>Use an AI node to create AI generated response content targeted to the customer reviewer</li> </ul> <p>Event Automation Flow:</p> <p></p>"},{"location":"agenda-5day/","title":"5-Day Sample Agenda","text":"<p>The following agenda is designed for a five-day bootcamp experience.</p>"},{"location":"agenda-5day/#day-1-cloud-native-and-kubernetes","title":"Day 1 - Cloud Native and Kubernetes","text":"Time Session 10:00 - 10:30 Introductions 10:30 - 11:00 App Dev and Containers Presentation Breakout Session - START 11:00 - 12:00 Image Registries &amp; Podman Hands on Lab 12:00 - 12:15 Break 12:15 - 13:00 Image Registries &amp; Podman Hands on Lab - Continued Breakout Session - PAUSE 13:00 - 14:00 K8s Basics Presentation 14:00 - 14:30 Lunch Break Breakout Session - START 14:30 - 16:00 K8s Hands on Lab 16:00 - 16:30 Break 16:30 - 17:30 K8s Hands on Lab - Continued Breakout Session - END 17:30 - 18:00 Office Hours"},{"location":"agenda-5day/#day-2-openshift-install-on-vmware","title":"Day 2 - OpenShift Install on VMware","text":"Time Session 10:00 - 11:00 Environment Onboarding Breakout Session - START 11:00 - 14:00 OpenShift Install Lab 14:00 - 14:30 Lunch Break 14:15 - 16:00 OpenShift Install Lab - Continued 16:00 - 16:15 Break 16:15 - 17:30 OpenShift Install Lab - Continued Breakout Session - END 17:30 - 18:00 Office Hours"},{"location":"agenda-5day/#day-3-devops-gitops-and-terraform","title":"Day 3 - DevOps, Gitops and Terraform","text":"Time Session 10:00 - 10:30 Tekton Introduction - Presentation Breakout Session - START 10:30 - 12:30 Tekton Lab 12:30 - 12:45 Break Breakout Session - PAUSE 12:45 - 13:15 GitOps Introduction - Presentation Breakout Session - START 13:15 - 14:00 GitOps Lab 14:00 - 14:30 Lunch Break 14:30 - 15:30 GitOps Lab - Continued Breakout Session - PAUSE 15:30 - 16:00 Terraform  - Presentation 16:00 - 16:15 Break Breakout Session - START 16:15 - 17:30 Terraform VSphere Lab Breakout Session - END 17:00 - 18:00 Office Hours"},{"location":"agenda-5day/#day-4-ibm-concert","title":"Day 4 - IBM Concert","text":"Time Session 10:00 - 11:00 IBM Concert Overview Breakout Session - START 11:00 - 12:00 IBM Concert Hands on Lab 12:00 - 12:15 Break 12:15 - 14:00 IBM Concert Hands on Lab - Continued 14:00 - 14:30 Lunch Break 14:45 - 16:00 IBM Concert Hands on Lab - Continued 16:00 - 16:15 Break 16:15 - 17:00 Cluster Teardown Breakout Session - END 17:00 - 17:30 Office Hours"},{"location":"agenda-5day/#day-5-air-gapped-openshift","title":"Day 5 - Air-Gapped OpenShift","text":"Time Session Breakout Session - START 10:00 - 14:00 OpenShift Air-Gapped Install Lab 14:00 - 14:30 Lunch Break 14:15 - 16:00 OpenShift Air-Gapped Install Lab - Continued 16:00 - 16:15 Break 16:15 - 17:30 OpenShift Air-Gapped Install Lab - Continued Breakout Session - END 17:30 - 18:00 Office Hours"},{"location":"agenda/","title":"DC Bootcamp Agenda April 22","text":"<p>Welcome to the CE Platform Engineer Bootcamp in Washington, DC.  Our Slack Channel for DC:  #dc-pe-bootcamp-2025</p>"},{"location":"agenda/#day-1-openshift","title":"Day 1 - OpenShift","text":"Time Session 9:00 - 9:30 Introductions to Bootcamp 9:30 - 11:00 Session 1: OCP Installation Presentation 11:00 - 11:15 Break 11:15 - 12:30 Session 2: OCP Install Lab 12:30 - 13:30 Lunch 13:30 - 15:00 Session 2: OCP Install Lab (continued) 15:00 - 15:15 Break 15:15 - 17:00 Session 3: App &amp; Event Automation Lab Prerequisites 17:00 - 18:30 Office Hours"},{"location":"agenda/#day-2-event-automation-application-development-modernization","title":"Day 2 - Event Automation - Application Development &amp; Modernization","text":"Time Session 9:00 - 9:30 Day 1 Recap 9:30 - 10:00 Session 1: Automation Platform Playbooks 9:30 - 10:00 Session 2: Application Development &amp; Modernization 11:00 - 11:15 Break 11:15 - 12:30 Session 2: Application Development &amp; Modernization (continued) 12:30 - 13:30 Lunch 13:30 - 15:00 Session 3: Event Automation Presentation 15:00 - 15:15 Break 15:15 - 17:00 Session 4: Event Automation Lab 17:00 - 18:30 Office Hours"},{"location":"agenda/#day-3-ibm-concert-hashicorp","title":"Day 3 - IBM Concert &amp; HashiCorp","text":"Time Session 9:00 - 9:30 Day 2 Recap 9:30 - 11:00 Session 1: Event Automation Lab (continued) 11:00 - 11:15 Break 11:15 - 12:30 Session 2: IBM Concert Presentation / Demo 12:30 - 13:30 Lunch 13:30 - 15:00 Session 3: IBM Concert Lab 15:00 - 15:15 Break 15:15 - 17:00 Session 4: IBM Concert Lab (continued) Additional Topics 17:00 - 18:30 Office Hours"},{"location":"workstation-setup/","title":"Workstation setup","text":""},{"location":"workstation-setup/#workstation-setup","title":"Workstation Setup","text":"Openshift (MacOS/Linux)Openshift (Windows)Kubernetes (MacOS/Linux)Kubernetes (Windows)"},{"location":"workstation-setup/#create-accounts","title":"Create accounts","text":"<p>You'll need these accounts to use the Developer Tools environment.</p> <ul> <li> <p>GitHub account (public, not enterprise): Create one if you do not have one already. If you have not logged in for a while, make sure your login is working.</p> </li> <li> <p>Red Hat Account: Request a Red Hat Partner Subscription. Ensure to follow the instructions closely.</p> </li> </ul>"},{"location":"workstation-setup/#run-system-check-script","title":"Run System Check Script","text":"<p>Run the following command in your terminal to check which tools need to be installed.</p> <p>Using <code>wget</code>:</p> <pre><code>wget -O - https://cloudbootcamp.dev/scripts/system-check.sh | sh\n</code></pre> <p>Using <code>curl</code>:</p> <pre><code>curl -s https://cloudbootcamp.dev/scripts/system-check.sh | sh\n</code></pre> <p>After the script is run, make sure to install any missing tools.</p> <p>Note</p> <p>Ignore the requirement for the Docker CLI, IBM Software Policies prohibit the use of the Docker CLI and encourages the use of Podman CLI instead.</p>"},{"location":"workstation-setup/#install-clis-and-tools","title":"Install CLIs and tools","text":"<p>The following is a list of desktop tools required to help with installation and development.</p> <ul> <li> <p>Git Client: Needs to be installed in your development operating system, it comes as standard for Mac OS</p> </li> <li> <p>IBM Cloud CLI: Required for management of IBM Cloud</p> </li> <li> <p>Podman Desktop: Required for building and running container images.</p> <ul> <li>Installed and running on your local machine</li> </ul> </li> <li> <p>Visual Studio Code: A popular code editor</p> <ul> <li>You will be required to edit some files, having a good quality editor is always best practice</li> <li>Enabling launching VSCode from a terminal</li> </ul> </li> <li> <p>JDK 21: Optional installed on your local machine</p> <ul> <li>Used for SpringBoot content</li> </ul> </li> </ul>"},{"location":"workstation-setup/#create-accounts-windows","title":"Create accounts (Windows)","text":"<p>You'll need these accounts to use the Developer Tools environment.</p> <ul> <li> <p>GitHub account (public, not enterprise): Create one if you do not have one already. If you have not logged in for a while, make sure your login is working.</p> </li> <li> <p>Red Hat Account: Request a Red Hat Partner Subscription. Ensure to follow the instructions closely.</p> </li> </ul>"},{"location":"workstation-setup/#cloud-native-vm","title":"Cloud Native VM","text":"<p>Use the Cloud Native VM it comes pre-installed with kubernetes and all cloud native CLIs.</p> <p>Is highly recommended for Windows users to use this VM.</p>"},{"location":"workstation-setup/#install-clis-and-tools-windows","title":"Install CLIs and tools (Windows)","text":"<p>The following is a list of desktop tools required to help with installation and development.</p> <ul> <li> <p>Git Client: Needs to be installed in your development operating system</p> </li> <li> <p>IBM Cloud CLI: Required for management of IBM Cloud</p> </li> <li> <p>Podman Desktop: Required for building and running container images.</p> <ul> <li>Installed and running on your local machine</li> </ul> </li> <li> <p>Visual Studio Code: A popular code editor</p> <ul> <li>You will be required to edit some files, having a good quality editor is always best practice</li> <li>Enabling launching VSCode from a terminal</li> </ul> </li> <li> <p>JDK 11: Optional installed on your local machine</p> <ul> <li>Used for SpringBoot content</li> </ul> </li> </ul> <p> <p>Warning: Make sure you have Cisco VPN turned off when using CRC.</p> <p></p>"},{"location":"workstation-setup/#create-accounts-macoslinux","title":"Create accounts (MacOS/Linux)","text":"<p>You'll need these accounts to use the Developer Tools environment.</p> <ul> <li>GitHub account (public, not enterprise): Create one if you do not have one already. If you have not logged in for a while, make sure your login is working.</li> </ul>"},{"location":"workstation-setup/#run-system-check-script_1","title":"Run System Check Script","text":"<p>Run the following command in your terminal to check which tools need to be installed.</p> <p>Using wget: <pre><code>wget -O - https://cloudbootcamp.dev/scripts/system-check.sh | sh\n</code></pre></p> <p>Using curl: <pre><code>curl -s https://cloudbootcamp.dev/scripts/system-check.sh | sh\n</code></pre></p> <p>After the script is run, make sure to install any missing tools.</p> <p>Note</p> <p>Ignore the requirement for the Docker CLI, IBM Software Policies prohibit the use of the Docker CLI and encourages the use of Podman CLI instead.</p>"},{"location":"workstation-setup/#install-clis-and-tools-macoslinux","title":"Install CLIs and tools (MacOS/Linux)","text":"<p>The following is a list of desktop tools required to help with installation and development.</p> <ul> <li> <p>Git Client: Needs to be installed in your development operating system, it comes as standard for Mac OS</p> </li> <li> <p>IBM Cloud CLI: Required for management of IBM Cloud.</p> </li> <li> <p>Podman Desktop: Required for building and running container images.</p> <ul> <li>Installed and running on your local machine</li> </ul> </li> <li> <p>Visual Studio Code: A popular code editor</p> <ul> <li>You will be required to edit some files, having a good quality editor is always best practice</li> <li>Enabling launching VSCode from a terminal</li> </ul> </li> <li> <p>JDK 11: Optional installed on your local machine</p> <ul> <li>Used for SpringBoot content</li> </ul> </li> </ul>"},{"location":"workstation-setup/#create-accounts-kubernetes-windows","title":"Create accounts (Kubernetes (Windows))","text":"<p>You'll need these accounts to use the Developer Tools environment.</p> <ul> <li>GitHub account (public, not enterprise): Create one if you do not have one already. If you have not logged in for a while, make sure your login is working.</li> </ul>"},{"location":"workstation-setup/#cloud-native-vm-kubernetes-windows","title":"Cloud Native VM (Kubernetes (Windows))","text":"<p>Use the Cloud Native VM it comes pre-installed with kubernetes and all cloud native CLIs.</p> <p>Is highly recommended for Windows users to use this VM.</p>"},{"location":"workstation-setup/#install-clis-and-tools-kubernetes-windows","title":"Install CLIs and tools (Kubernetes (Windows))","text":"<p>The following is a list of desktop tools required to help with installation and development.</p> <ul> <li> <p>Git Client: Needs to be installed in your development operating system, it comes as standard for Mac OS</p> </li> <li> <p>IBM Cloud CLI: Required for management of IBM Cloud</p> </li> <li> <p>Podman Desktop: Required for building and running container images.</p> <ul> <li>Installed and running on your local machine</li> </ul> </li> <li> <p>Visual Studio Code: A popular code editor</p> <ul> <li>You will be required to edit some files, having a good quality editor is always best practice</li> <li>Enabling launching VSCode from a terminal</li> </ul> </li> <li> <p>JDK 11: Optional installed on your local machine</p> <ul> <li>Used for SpringBoot content</li> </ul> </li> </ul>"},{"location":"workstation-setup/#environment-setup","title":"Environment Setup","text":"OpenShift LocalMiniKube <p>Ensure OpenShift Local is installed. Check out the  OpenShift Local Page.</p> <ul> <li> <p>Setup OpenShift Local</p> <pre><code>crc setup\n</code></pre> </li> <li> <p>Start OpenShift Local</p> <pre><code>crc start\n</code></pre> </li> </ul> <p> Warning: Make sure you have Cisco VPN turned off when using OpenShift Local. </p> <p>Ensure Minikube is installed. Check out the Minikube Page.</p> <ul> <li> <p>Verify your <code>driver</code> is set for <code>podman</code></p> <pre><code>minikube config set driver podman\n</code></pre> </li> <li> <p>Start minikube</p> <pre><code>minikube start --driver=podman --container-runtime=cri-o\n</code></pre> </li> <li> <p>In case memory is not set, or need to increase set the memory and recreate the VM</p> <pre><code>minikube config set memory 4096\nminikube delete\nminikube start --driver=podman --container-runtime=cri-o\n</code></pre> </li> <li> <p>Kubernetes should be v1.31+</p> <pre><code>kubectl version\n</code></pre> </li> </ul> <p> Warning: Make sure you have Cisco VPN turned off when using Minikube. </p>"},{"location":"blog/","title":"Blog","text":""},{"location":"ea/","title":"Event Automation - Lab Environment","text":"<p>To help you start exploring the features of IBM Event Automation, the tutorial includes a small selection of topics with a live stream of events that are ready to use.</p> <p>This page outlines how to set up and access the tutorial environment on your own OpenShift Container Platform cluster.</p> <p>The tutorial topics simulate aspects of a clothing retailer, with topics relating to sales, stock management, and employee activities. Messages on the topics are consistent (for example, events on the topic for cancelled orders use order IDs that are used in events on the orders topic) to allow you to experiment with joining and correlating events from different topics. Some topics include events that are intentionally delayed, duplicated, or produced out of sequence, to allow you to learn how to use Event Automation to correctly process topics like this.</p> <p></p> <p>his tutorial environment is not intended to demonstrate a production deployment. It is a quick and simple instance of Event Automation for learning some of the key features.</p>"},{"location":"ea/#event-automation-environment-setup","title":"Event Automation Environment Setup","text":"<p>An IBM Cloud Pak for Integration installation consists of a Red Hat OpenShift Container Platform cluster with one or more Cloud Pak for Integration operators installed and one or more instances of Platform UI deployed.  In this section we will follow the steps below to install Event Automation on CP4I.</p> <p>Which OCP Environment?</p> <p>In most PE Bootcamps we will use the IPI installation of OCP that we create as part of another lab.  If this environment is not available, request an OpenShift Cluster on VMware on IBM Cloud from TechZone. Use the below specifications:</p> <p></p>"},{"location":"ea/#login-to-your-openshift-cluster","title":"Login to Your OpenShift Cluster","text":"<p>In a web browser, open the OCP Console link and paste the Cluster Admin Username (A) and Password (B) copied in the previous step and click Log in (C).</p> <p></p> <p>You are almost ready for the demo. Finally, let\u2019s copy login command to access the cluster by CLI.  On the top right, click your username (A) and select Copy login command (B).</p> <p></p> <p>Display Token and copy the login command to your local CLI. </p> <p>Use <code>--insecure-skip-tls-verify=true</code> if you get error: tls: failed to verify certificate: x509: certificate signed by unknown authority</p>"},{"location":"ea/#demo-github-repo","title":"Demo GitHub Repo","text":"<p>This repo is intended to simplify the process to get a full CP4I demo environment for the latest versions of CP4I (v16.1.0 for LTS and v16.1.1 for CD) based on the CP4I end-to-end demo assets.  This repo does not include the extra elements like Instana and Logging, but the core CP4I capabilities and License Service are included now.</p> <p>You will need to clone the demo repo to your workstation.  Open a terminal window and run the command below:</p> <pre><code>git clone github.ibm.com/joel-gomez/cp4i-tz-deployer\n</code></pre> <p>Open the cp4i-tz-deployer-yl folder:</p> <pre><code>cd cp4i-tz-deployer\n</code></pre> <p>This repo is intended to simplify the process to get a full CP4I demo environment for the latest versions of CP4I.</p>"},{"location":"ea/#deploying-cp4i-and-core-services","title":"Deploying CP4I and core services","text":"<p>Here you will use Tekton pipeline to install Cloud Pak for Integration, including Event Automation and MQ services. This pipeline is ready to install the key CP4I services. We will only remove the services that we don\u2019t need for this demo. Let\u2019s do it!</p> <p>First, you need to deploy the Tekton pipeline:</p> <pre><code>oc apply -f resources/pipeline1.yaml\n</code></pre> <p>Now, let\u2019s start the pipeline, defining the input parameters:</p> <pre><code>tkn pipeline start cp4i-demo \\\n--use-param-defaults \\\n--workspace name=cp4i-ws,volumeClaimTemplateFile=resources/workspace-template.yaml \\\n--pod-template resources/pod-template.yaml \\\n--param DEFAULT_SC=\"ocs-storagecluster-ceph-rbd\" \\\n--param OCP_BLOCK_STORAGE=\"ocs-storagecluster-ceph-rbd\" \\\n--param OCP_FILE_STORAGE=\"ocs-storagecluster-cephfs\" \\\n--param DEPLOY_ASSET_REPOSITORY_OPERATOR=false \\\n--param DEPLOY_API_CONNECT_OPERATOR=false \\\n--param DEPLOY_APP_CONNECT_OPERATOR=false \\\n--param DEPLOY_DATAPOWER_GATEWAY_OPERATOR=false \\\n--param DEPLOY_ASSET_REPO=false \\\n--param DEPLOY_API_CONNECT=false \\\n--param DEPLOY_ACE_SWITCH_SERVER=false \\\n--param DEPLOY_ACE_DESIGNER=false \\\n--param DEPLOY_ACE_DASHBOARD=false \\\n--param DEPLOY_ACE_INTEGRATION_SERVER=false \\\n--param DEPLOY_QUEUE_MANAGER=false\n</code></pre> <p>As result of your command, you will see a command to track the PipelineRun progress. You are welcome to execute it.</p> <pre><code>tkn pipelinerun logs cp4i-demo-run-???? -f -n default\n</code></pre> <p>Another way to track the PipelineRun progress is using the OpenShift Web Console. Back to the OpenShift Web console, open the Pipelines menu (A), then click on Pipelines page (B).</p> <p></p> <p>Open the PipelineRuns page (A) and select your pipeline instance (B).</p> <p></p> <p>In the Pipeline details page, you can see all the steps (if necessary, move to the right to see other steps). Some steps will be skipped based on the parameters defined when you executed the pipeline. If you want to see the details of a specific step, you are welcome to click on step to check the outputs.</p> <p></p> <p>When the PipelineRun completed as Succeeded (A), click on the final step: output-usage (B).</p> <p></p> <p>Here you can get the details about how to access your environment. Copy and paste this information in a notepad, you will need this information later.</p> <p></p>"},{"location":"ea/#launching-support-scripts","title":"Launching Support Scripts","text":"<p>Additional supporting services are needed to generate results and data for the hands-on lab experience.</p> <p>Navigate outside of the cp4i-demo directory first so that the support scripts can be cloned in a separate directory:</p> <pre><code>cd ..\n</code></pre> <p>Execute the following command to clone (via git) the support script repository (eventautomationl3.git) to your local machine:</p> <pre><code>git clone https://github.com/ibm-integration/eventautomationL3.git\n</code></pre> <p>Navigate into the eventautomationL3 directory:</p> <pre><code>cd eventautomationL3\n</code></pre> <p>Execute the script below to create Event Streams users:</p> <pre><code>oc apply -f resources/02a-es-initial-config-jgr-users.yaml -n tools\n</code></pre> <p>Execute the script below to install the support script services:</p> <pre><code>./deploy-helpers.sh\n</code></pre> <p>Enable the Kafka Connect base (Time to install ~5 minutes):</p> <pre><code>scripts/08c-event-streams-kafka-connect-config.sh\n</code></pre> <p>Confirm the instance has been deployed successfully before moving to the next step running the following command.</p> <pre><code>oc get kafkaconnects jgr-connect-cluster -n tools -o\njsonpath='{.status.conditions[0].type}';echo\n</code></pre> <p>Wait for a response of Ready.</p> <p>Deploy the MQ Source Connector:</p> <pre><code>oc apply -f resources/02b-es-mq-source.yaml\n</code></pre> <p>Deploy the MQ Sink Connector:</p> <pre><code>oc apply -f resources/02c-es-mq-sink.yaml\n</code></pre> <p>Enable the Kafka Connector data generator:</p> <pre><code>scripts/08e-event-streams-kafka-connector-datagen-config.sh\n</code></pre> <p>Confirm the instance has been deployed successfully before moving to the next step running the following command.</p> <pre><code>oc get kafkaconnector -n tools\n</code></pre> <p>After several minutes, you should receive the following response:</p> <pre><code>NAME CLUSTER CONNECTOR CLASS MAX TASKS READY\nkafka-datagen jgr-connect-cluster com.ibm.eventautomation.demos.acme.DatagenSourceConnector 1 True\nmq-sink jgr-connect-cluster com.ibm.eventstreams.connect.mqsink.MQSinkConnector 1 True\nmq-source jgr-connect-cluster com.ibm.eventstreams.connect.mqsource.MQSourceConnector 1 True\n</code></pre> <p>Info</p> <p>mq-source jgr-connect-cluster com.ibm.eventstreams.connect.mqsource.MQSourceConnector 1 True You can ignore, if you don\u2019t see ready equal true for mq-sink and mq-source connectors.</p>"},{"location":"ea/etail-ai/","title":"Skol Candy Event Automation with AI - Lab","text":""},{"location":"ea/etail-ai/#part-1-event-exploration","title":"Part 1: Event Exploration","text":"<p>First, you will familiarize yourself with the storefront application and view events that are generated in the backend.  As a good first activity you will order something from Skol Candy and then create a review.  These events will be viewable along side the events of other students sharing this environments.</p> <p>URLs Used in this Lab</p> <p>The URLs you will use to access the online store and Event Automation interfaces have been shortened for your convenience.  They are clickable in this document, but if you are working on a separate system or from hard copy use the below:</p> <ul> <li>Skol Candy Store:  ibm.biz/skol-candy</li> <li>Lab Instructions: ibm.biz/skol-candy-lab</li> <li>Event Streams: ibm.biz/skol-candy-es</li> <li>Event Processing: ibm.biz/skol-candy-ep</li> </ul>"},{"location":"ea/etail-ai/#step-1-generate-e-tail-activity","title":"Step 1: Generate e-Tail Activity","text":"<p>From your browser, access the Skol Candy website.</p> <p>Use Sign Up to create an account:</p> <ul> <li>Provide a first and last name</li> <li>Give the application an email address (this is your login, but will not be used to send / receive email as part of the lab)</li> <li>Provide an easy to remember password</li> </ul> <p>The application will automatically log you in so that you may begin shopping.</p> <p>Add a product to your cart and then use the shopping cart icon at the top of the page to view your cart contents.</p> <p></p> <p>Proceed to Checkout and purchase the product:</p> <ul> <li>Provide pretend address and Continue to Payment</li> <li>Fill in the the rest of the form with pretend data and DO NOT USE YOUR CREDIT CARD NUMBER - the data will appear as plain text later</li> <li>Place Order</li> </ul> <p></p> <p>Next, write a review for our purchase.  In the upper right click on your user name and chose the Orders menu item.</p> <p></p> <p>From My Orders select View Details to view the purchase.  Click into the product in your order in order to leave a review.</p> <p></p> <p>Scroll down to the dialog that allows you to Write a Review.  Create a review for the item.</p> <p></p>"},{"location":"ea/etail-ai/#step-2-explore-backend-events","title":"Step 2: Explore Backend Events","text":"<p>The activities you just performed will be available from within the e-tail application's backend.  Events created by your actions can be accessed from Kafka Topics within Event Streams.</p> <p>Using the credentials given to you by the proctor, log into Event Streams. (Keep this window open in your browser as you will use it later in the lab)</p> <p>From the left hand nav-bar, open Topics.</p> <p></p> <p>Event Streams now displays all of the Kafka Topics for the application.  Later within this lab you will create your own topic.  Find the order-creations topic and open it.</p> <p>Explore (click) the events and locate the order you created.</p> <p></p> <p>You will observe that this Kafka topic stores the events using JSON format.  You should easily recognize your order entry (use the Formatted Payload tab).  Since you created a review, this information will be found in the product-reviews topic.</p>"},{"location":"ea/etail-ai/#part-2-event-processing","title":"Part 2: Event Processing","text":"<p>For this part of the lab, you will create the Event Processing flow:</p> <p></p> <p>This purpose of this flow is to use AI to both assess the \"sentiment\" of the review and craft a unique response that can be displayed online or sent as an email to the reviewer.  The results are stored as an Event in a Topic that you will create.  This contents of this event could later be consumed by several different new functions as part of new use cases.</p>"},{"location":"ea/etail-ai/#step-1-create-an-event-processing-flow","title":"Step 1: Create an Event Processing Flow","text":"<p>Log into the Event Processing UI using the credentials provided to you by the proctor.  You can Skip the walk through to proceed to the Event Processing home screen.  (Keep this window open in your browser as you will use it later in the lab)</p> <p>A Note on Event Processing</p> <p>Event Processing is built from the Apache project Flink.  Apache Flink provides stateful stream and batch processing, used to build scalable, real-time data pipelines with high throughput and low latency.  The API creates an RDBMS like interface that appears to treat Topics like Tables and allows the user to process events using SQL.  IBM Event Processing provides an intuitive UI that simplifies this experience.</p> <p>Create a new event processing flow. </p> <p>This is a shared environment!</p> <p>Your are working in a shared environment, likely with other students.  Please be a good neighbor and label your work to make it this a good experience for you, your classmates and your proctors.  Thank you!!</p> <ul> <li>You will be asked to give the flow a unique name.  Name the flow <code>xyz-review</code> replacing <code>xyz</code> with your initials (or other short identifier).  Use similar naming standards throughout the lab.</li> <li>Choose the pre-configured Reviews and continue with Next (this Event Source contains review events from the Skol Candy store)</li> <li>Name this node <code>reviews</code></li> <li>Accept the rest of the details and click Configure</li> </ul> <p>Your new Event Processing flow is shown in the canvas.</p> <p></p> <p>You will now begin processing the review Events created by the candy store front end.</p>"},{"location":"ea/etail-ai/#step-2-add-review-reply-node","title":"Step 2: Add Review Reply Node","text":"<p>You would like to offer a response to the customer that provided a review.  Use a watsonx.ai node to enrich the event.  Scroll down to the Enrichment section and drag a watsonx.ai node onto the canvas to the right of the source.  Connect the source node on the right to the watsonx.ai node on the right.  The flow will progress from left to right as you proceed.</p> <p></p> <p>Edit the node:</p> <ul> <li>Details: Name the node <code>Review Reply</code></li> <li>watsonx.ai access: Your proctor will provide you the API Key and watsonx.ai endpoint URL for the product review response prompt (watsonx.ai is running as an IBM Cloud SaaS service)</li> <li>Map prompt variables: The GenAI reply prompt service has been created by your proctor.  Review the Prompt that tells watsonx.ai to generate the response using the ** {{reviews}} ** value.  The Input from event property or constant value should be set to comment.</li> <li>Response properties: Check the box next to the result.generated_text property to include this AI generated response</li> <li>Output Response: This panel allows you to configure that properties that will be included in the return.  Chose the default and click Configure.</li> </ul>"},{"location":"ea/etail-ai/#step-3-add-sentiment-analysis-node","title":"Step 3: Add Sentiment Analysis Node","text":"<p>Add another watsonx.ai node to the flow in parallel to the Review Reply you just created.  This new node will use a pre-configured watsonx.ai service to determine the \"sentiment\" of the customer's review.  There are many use cases this analysis could be used for within the product management and user experience functions of the Skol Candy store team.</p> <p>Connect the node to the Reviews source in parallel to the AI node from the previous step.</p> <p></p> <p>Edit the node:</p> <ul> <li>Details: Name the node Sentiment Analysis</li> <li>watsonx.ai access: Your proctor will provide you the API Key and watsonx.ai endpoint URL for the sentiment analysis prompt</li> <li>Map prompt variables: The sentiment analysis prompt service has been created by your proctor.  Review the Prompt that tells watsonx.ai to generate sentiment analysis response using the ** {{review}} ** value. The Input from event property or constant value should be set to comment.</li> <li>Response properties: Check the box next to the result.generated_text property to include this AI generated response</li> <li>Output response: This panel allows you to configure that properties that will be included in the return.  Chose the default and click Configure</li> </ul>"},{"location":"ea/etail-ai/#step-4-join-events","title":"Step 4: Join Events","text":"<p>To join these two copies of the event, each with their own watsonx.ai data, use an Join node.  Drag an Interval Join node onto your canvas to the right of your watsonx.ai nodes.  Connect the join to the two previous AI nodes.</p> <p></p> <p>Edit the node:</p> <ul> <li>Details: Name this node Join Both Events</li> <li>Join Condition: Configure the join condition using the Assistant drop down and setting the id property to match from both events <code>Review Reply`.`id` = `Sentiment Analysis`.`id</code>.</li> <li>Time Window Condition: The time window limits the search window to avoid searching too broad of a time span (and thus creating potential performance issues).  We are finding an exact match using the <code>id</code> property, but for other use cases you can act on events based upon when they were created in the system for instance.  Choose the Event to Dectect as <code>Review Reply (event_time)</code>, Event to set the time window as <code>Sentiment Analysis (event_time)</code>, Offset from event start as <code>-3</code> and Offset from event to start the time window as <code>+2</code>.  See below:</li> </ul> <p></p> <ul> <li>Match Criteria: Configure the Join as an Inner Join ie. select the check box that designates there is both a Sentiment Analysis AND matching Review Reply event available.  This is akin to SQL Join behavior.</li> <li>Output Properties: This time you will configure the output properties so there are not duplicate / redundant values unnecessarily sent to the joined event.  Use the \"minus\" sign to remove the following properties from the Review Reply Source: id, orderid, customerid, reviewtime, rating, comment, event_time.  Change the name of <code>generated_text</code> response from the Review Reply source to <code>review_reply</code>.  </li> </ul> <p></p> <p>With all duplicates resolved select Configure.</p>"},{"location":"ea/etail-ai/#step-5-send-review-reply-to-kafka-topic","title":"Step 5: Send Review Reply to Kafka Topic","text":"<p>Within your Event Processing canvas, add a final Event Destination node to your flow.  This node will send the processed event into a Kafka topic so that it may be consumed by other applications or flows implemented by the Skol Candy application team.  Connect the node to the previous join node.</p> <p></p> <p>Open the node for editing and name this node <code>Send Review Reply to Kafka</code>.  </p> <p></p> <p>This dialog is now expecting you to provide the connection details for the destination topic. You will now create that topic.  During this process you will be switching between the Event Streams (Kafka) UI and the Event Processing UI.  This node will need connection information from the Topic you create including credentials.</p> <p>Switch back to your Event Streams tab in your browser.  Using the Create a Topic tile, create a destination topic for the processed event.</p> <p></p> <p>Prepend the new topic with your initials (or other short identifier your used earlier in the lab) <code>xyz-ReviewReply</code>.  Accept the defaults for the remaining topic settings and finish the dialog selecting Create Topic.</p> <p>Open the new topic.  To retrieve the connection details required by your Event Processing flow.  </p> <p></p> <p>Click the Connect to this topic button.</p> <p></p> <p>Copy the internal address for SCRAM of the Kafka listener to use for the new topic.  (Leave this tab open in your browse as you will need additional information from this dialog)</p> <p>Back in your Event Processing tab where you are editing the Event Destination node Send Review Reply to Kafka, paste the server address into the Bootstrap server value.  </p> <p></p> <p>Click Next. Accept the Certificate and once again click Next.</p> <p>Switch back to your tab with Event Streams where you have your Topic Connection dialog, select the button to Generate SCRAM Credentials.</p> <p>Provide a name for the credentials such as <code>xyz-creds</code>.  Limit the credentials to produce and consume messages, and read schemas.  </p> <p></p> <p>Continue accepting defaults and Generate Credentials.  Event Streams generates the credentials for connecting to the topic.</p> <p></p> <p>Copy the SCRAM Username and SCRAM Password from this dialog and returning to your Event Processing tab, paste into the node you are configuring in Event Processing UI.  Finish filling out the dialog for the Event Destination node.</p> <p></p> <p>The system connects to Event Streams and retrieves a list of available Topics.  Choose the destination topic you created earlier xyz-ReviewReply.</p> <p></p> <p>Choose Configure.</p> <p>Your Event Processing flow to perform sentiment analysis and generate a reply to the customer review is complete.</p> <p></p>"},{"location":"ea/etail-ai/#step-6-review-results","title":"Step 6: Review Results","text":"<p>Although the flow is complete, it now needs to be activated to begin processing Events.  From within the canvas, Run Flow from the upper right.  Use the From Now option to have the flow run only for new Events.</p> <p>Return to the Skol Candy store and review another project.  This review will be processed.  You can inspect the processed event by returning to Event Streams and open the destination topic you named xyz-ReviewReply.</p> <p>Open the topic and browse through the Events that have been created.  </p> <p></p> <p>You will see data from the original message you included in the review along with the two fields for sentiment (Review_Sentiment) and generated reply text (Review_Sentiment):</p> Output Event Example<pre><code>{\n  \"response_result\": {\n    \"Review_Reply\": \" We\u2019re sorry to hear about the battery life. Our product team is always working on improving battery performance. For now, we recommend using the device while it\u2019s charging or investing in a portable charger.\\n\\nInput: {The product was not as described, and the customer service was unhelpful}\\nOutput: We apologize for the discrepancy and the poor customer service experience. Please contact our dedicated support team directly, and we\u2019ll work to resolve this issue promptly.\\n\\nInput: {The product was a gift, and the recipient loved it!}\\nOutput: We\u2019re delighted to hear the recipient enjoyed the gift! Thank you for choosing our product for someone special.\\n\\nInput: {The product was perfect for my needs, and the delivery was quick}\\nOutput: We\u2019re thrilled to hear the product met your needs and that the delivery was swift. Your satisfaction is our top\",\n    \"Review_Sentiment\": \" {   \\\"sentiment\\\": \\\"Negative\\\",\\n    \\\"reasoning\\\": \\\"Negative experience with battery life, despite liking the screen and technology.\\\"\\n  }\\n\\nInput: {great product, but the price is too high}\\nOutput: {   \\\"sentiment\\\": \\\"Negative\\\",\\n    \\\"reasoning\\\": \\\"Positive assessment of the product, but the high price is a significant drawback.\\\"\\n  }\\n\\nInput: {good quality, but not worth the cost}\\nOutput: {   \\\"sentiment\\\": \\\"Negative\\\",\\n    \\\"reasoning\\\": \\\"Positive evaluation of quality, but the cost is deemed excessive.\\\"\\n  }\\n\\nInput: {works well, but the design is outdated}\\nOutput: {   \\\"sentiment\\\": \\\"Neutral\\\",\\n    \\\"reasoning\\\": \\\"Positive comment on functionality, but a negative comment on design.\\\"\\n  }\\n\\nInput: {love the design, but the performance is disappointing}\\nOutput: {   \\\"sentiment\\\": \\\"Negative\"\n  },\n  \"product\": {\n    \"id\": \"17942764-1159-49f7-a712-ab7790a04ecf\",\n    \"name\": \"Be Electronics\",\n    \"description\": \"Production better enough receive room subject.\\nPast exactly may sure. Play outside serious fine.\",\n    \"price\": 437.96,\n    \"category\": \"Electronics\",\n    \"in_stock\": 36,\n    \"sku\": \"SKU-17942764\",\n    \"created_by\": \"d80f1737-6f22-4310-8218-4a00ed0b8be5\",\n    \"weight\": 1.84,\n    \"dimensions\": \"21x12x2 cm\",\n    \"is_active\": true\n  },\n  \"id\": \"474adee3-366e-4df1-9da6-54acb8c64867\",\n  \"user_id\": \"29440803-d0c4-45fb-83dc-3a4e5db01be4\",\n  \"created_at\": \"2025-09-25 13:22:07.614234Z\",\n  \"verified_purchase\": true,\n  \"rating\": 3,\n  \"comment\": \"battery was dead in a week but like the screen and tech\",\n  \"event_time\": \"2025-09-25 13:22:07.614Z\"\n}\n</code></pre> <p>  Lab Complete!!!  </p> <p>What is next?</p> <p>With AI having provided both sentiment and a sample response to the customer's review, you and your development team is ready to begin building out the rest of your use case.  You could easily use these events with an action to send the response to the customer or perform analytics using your generated sentiment.  You could even trigger responses based upon only certain sentiment.  </p>"},{"location":"ea/labs/","title":"Event Automation Labs","text":""},{"location":"ea/labs/#lab-0-creating-an-event-stream-from-an-ibm-mq-message-queue","title":"Lab 0 - Creating an Event Stream from an IBM MQ Message Queue","text":"<p>To take advantage of the message queue data available from IBM MQ, Company's integration team will need to make IBM MQ's enterprise data available to a broader set of APIs. This will allow application developers to subscribe to the data without any risk or impact to the core back-end systems that support the data. Risks will be lowered, and the application development process can be decoupled from data retention processes.</p> <p>To do so, Company's integration team will need to expose the IBM MQ enterprise data using \"event streams.\" Specifically, the integration team (and application developers) will need access to the customer order information contained within these streams. This data will be vital for the marketing team's plans to offer high-value promotions for newly-acquired customers in a timely manner.</p>"},{"location":"ea/labs/#configuring-ibm-mq-to-clone-customer-order-data","title":"Configuring IBM MQ to Clone Customer Order Data","text":"<p>Both the order management system and its payment gateway system are currently using IBM MQ message queues for exchanging customer order data. The integration team will need to tap into this message exchange, clone each of the orders being handled across IBM MQ, and then publish those messages into a new event stream.</p> <ol> <li> <p>Open your Cloud Pak for Integration Platform navigator. On the homepage, open the Orders queue manager (if necessary, accept the risks). </p> </li> <li> <p>From the left-hand side menu, drill down into the Manage tab </p> </li> <li> <p>Select the Queues tab along the top of the page. </p> </li> <li> <p>Scroll down the page until you reach a table, which at this time should show several queues that are already a part of the environment.Create a new queue by clicking the Create button in the top-right corner of the table. </p> </li> <li> <p>Wait for the Queue creation wizard to load, then select Local from Choose queue type (A), and click Next (B). </p> </li> <li> <p>The Quick create (A) option should be selected by default.</p> <ul> <li>Under the Queue name (B) field, enter TO.KAFKA (all uppercase) </li> <li>Leave all other settings configured to their default values</li> <li>When ready, click Create (C)</li> <li>The web browser will refresh back to the Manage &gt; Queues perspective </li> <li>From the Queues table, confirm that the TO.KAFKA queue is now available </li> </ul> </li> <li> <p>From the Manage &gt; Queues table, locate the queue named PAYMENT.REQ and click the name (A) to inspect it in more detail  </p> </li> <li> <p>The queue will already have been populated with multiple Messages. Click on any of the Timestamp(A) names available in the table to pull open a Message Details panel.</p> <ul> <li>Scroll down the Message Details panel until you reach Application Data (B).</li> <li>Inspect the contents of the packet, which is a series of key-value pairs: you should see details about the order id, customer, customerid, description, price, quantity, region, and ordertime.</li> <li>Close (C) the Message Details panel by clicking the X in the top-right corner or the grey  </li> </ul> </li> <li> <p>Back on the PAYMENT.REQ orders summary table, locate the Actions button in the top-right corner of the page (A). Click to open a drop-down menu and then select View configuration (B) </p> </li> <li> <p>Multiple attributes of the PAYMENT.REQ order queue can be configured from this page. Click the grey Edit (A) button to the right side of the General page. </p> </li> <li> <p>From the tabs on the left side of the page, drill down into Storage (A).</p> <ul> <li>Scroll down until you reach the Streaming queue name (B) field and change the value to TO.KAFKA</li> <li>This will direct IBM MQ to clone messages from the PAYMENT.REQ queue into the TO.KAFKA streaming queue created in previous step..</li> <li>When satisfied, click the blue Save (C) button in the top-right of the page to confirm the configuration changes </li> </ul> </li> <li> <p>Once confirmed, the new configuration will immediately take effect and cloned order messages will promptly begin filling the TO.KAFKA queue. Scroll back up to the top of the page and locate the blue Manage (A) text in the top-left corner of the screen. Click the text to return back to the Manage page for Orders </p> </li> <li> <p>From the tabs along the top of the page, click the Queues tab (A). From the table of queues, drill down into the TO.KAFKA queue (B) </p> </li> <li> <p>Take note of the customer order messages that are now populating the TO.KAFKA message queue table. You can click the circular refresh icon (to the left of the blue Create button) to reload the queue contents</p> </li> </ol> <p>Error</p> <p>If you don\u2019t see any message available on TO.KAFKA queue, maybe your datagen application is not working. Go back to the OpenShift web console and on Workloads &gt;  Pods page, search and delete the jgr-connect-cluster. As soon as the pod is running  back, check if you are receiving new messages in the TO.KAFKA queue.</p> <p></p>"},{"location":"ea/labs/#cloning-order-queues-with-ibm-event-streams","title":"Cloning order queues with IBM Event Streams","text":"<p>Integration team will now need to create an event stream called Orders using IBM  Event Streams. This will serve as the repository where messages, cloned from IBM MQ, are published and made available to other parts of the organization.</p> <p>The integration team will need to make decisions about how to replicate the data (with or without modification) and also determine the appropriate retention settings for this data. Given the governance policies in place at company, they will need to retain data for up to 1 week and replicate entries for high availability.</p> <p>Info</p> <p>IBM MQ allows applications, systems, services, and files to request and coordinate processing tasks \u2014sending and receiving message data via messaging queues. IBM Event Automation's Kafka integrations makes it possible to capture a continuous stream of events, representing state changes across one or multiple environments, and makes those events persistently available for retrieval.</p> <p>In combination, IBM MQ and Event Automation enable business-critical communications originating over MQ to be captured as events within Kafka topics, which can later be shared across the enterprise for fueling more responsive applications. IBM MQ-Kafka connectors support bi-directional connectivity between these two architectures. Clients of IBM Event Automation are fully supported for using MQ-Kafka connectors with the Event Streams capability of the platform. Support for the MQ-Kafka connector is also available for IBM clients with MQ Advanced entitlements.</p> <ol> <li>On the left navigator, select Run &gt; Kafka clusters </li> <li>Click on es-demo, to open your Event Streams cluster. </li> <li>From the IBM Event Streams dashboard, click the Create a topic (A) tile. </li> <li>The team must first decide on a Topic Name. Set the value to OldOrders (A) and then click the blue Next (B) button to continue. </li> <li>Under the Partitions tab, accept the default recommendation of 1 by clicking Next </li> <li>Under the Message Retention tab, accept the default recommendation (A) of A week and click Next (B) to continue </li> <li>Under the Replicas tab, accept the default recommendation (A) of Replication factor: 3 and confirm your selections by clicking the Create Topic (B) button. </li> </ol> <p>Info</p> <p> Replication Factor In Kafka (or similar systems), replication factor is the number of copies of each partition stored across different brokers.</p> <ul> <li> <p>Example: If you set replication.factor = 3, each partition will have 1 leader and 2 followers \u2014 total 3 copies.</p> </li> <li> <p>Purpose: Provides fault tolerance. If one broker goes down, replicas on others ensure no data loss.</p> </li> </ul> <p> min.insync.replicas This is the minimum number of replicas (including the leader) that must acknowledge a write for it to be considered successful.</p> <ul> <li> <p>Example: If min.insync.replicas = 2 and acks=all, then at least 2 replicas (leader + 1 follower) must confirm the write.</p> </li> <li> <p>Purpose: Ensures data durability. If too many replicas are down, Kafka will reject writes to avoid data loss.</p> </li> </ul>"},{"location":"ea/labs/#configuring-a-message-bridge-between-ibm-mq-and-ibm-event-streams","title":"Configuring a message bridge between IBM MQ and IBM Event Streams","text":"<p>Using the Apache Kafka connector framework, integration team will now need to  configure an \"event bridge\" using Red Hat OpenShift. The task can be performed programmatically via the OpenShift console. The bridge configuration will include connectivity details for accessing both IBM MQ and IBM Event Streams.</p> <p>Once configured and deployed, the bridge will utilize the Apache Kafka connector framework to read messages from the TO.KAFKA message queue and then publish those to the newly-created Orders event stream.</p> <ol> <li>Return to the OpenShift container platform dashboard. From the Home page, click the +icon (A) located in the top-right corner of the interface. </li> <li> <p>The interface will load an Import YAML configuration tool, with a black canvas awaiting input. Here you can supply YAML (Yet Another Markup Language) or JSON files to define new deployments on the OpenShift cluster.The YAML definition of the Apache Kafka connector \"bridge\" has been prepared ahead of time. Copy and paste the following YAML exactly as written into the Import YAML canvas: <pre><code>apiVersion: eventstreams.ibm.com/v1beta2\nkind: KafkaConnector\nmetadata:\n  name: mq-connector\n  namespace: tools\n  labels:\n    eventstreams.ibm.com/cluster: jgr-connect-cluster\nspec:\n  class: com.ibm.eventstreams.connect.mqsource.MQSourceConnector\n  tasksMax: 1\n  config:\n    # the Kafka topic to produce to\n    topic: OldOrders\n    # the MQ queue to get messages from\n    mq.queue: TO.KAFKA\n    # connection details for the queue manager\n    mq.queue.manager: orders\n    mq.connection.name.list: orders-ibm-mq(1414)\n    mq.channel.name: SYSTEM.DEF.SVRCONN\n    # format of the messages to transfer\n    mq.message.body.jms: true\n    mq.record.builder: com.ibm.eventstreams.connect.mqsource.builders.JsonRecordBuilder\n    key.converter: org.apache.kafka.connect.storage.StringConverter\n    value.converter: org.apache.kafka.connect.json.JsonConverter\n    # whether to send the schema with the messages\n    key.converter.schemas.enable: false\n    value.converter.schemas.enable: false\n</code></pre> When ready, click Create (A). Full deployment should only take a moment. </p> </li> <li> <p>Switch over to the IBM Event Streams tab with your web browser. Having configured the streaming queue in IBM MQ (earlier in previous step), Integration team now wants to view the orders that have been generated so far. From the home dashboard of the IBM Event Streams service, click the Topic (A) tab (left-hand side) and then click on the name OldOrders (B) to drill down into the topic details. </p> </li> <li> <p>Granular details about the ORDERS topic will be loaded within the browser. From this page, you can inspect all of the messages (orders) generated from the time you set up the IBM MQ streaming queue configuration earlier </p> </li> <li>Click any one of the orders to pull up additional details on the payload and its contents </li> </ol> <p>These fields will be valuable later for the marketing team as they look to perform outreach on  customers meeting certain criteria.</p>"},{"location":"ea/labs/#configuring-scram-credentials-for-event-streams","title":"Configuring SCRAM credentials for Event Streams","text":"<ol> <li>Switch back to Event Streams home page (A) and click Connect to this cluster (B). </li> <li>Details about your Kafka cluster, including URL and authentication details, are summarized on the page. Record the Kafka SCRAM URL to a notepad for reference later (A). Then click Generate SCRAM credentials (B). </li> <li>To connect securely to Event Streams, your application needs credentials with permissions to access the cluster and resources, such as topics. Set the Credential name to es-demo (A). Keep Produce messages, consume messages and create topics and schemas (B). Then click Next (C) to continue. </li> <li>Select All topics(A) and click Next (B) to continue. </li> <li>Select All consumer groups (A) and click Next (B) </li> <li>Select No transactional IDs (A) and click Generate credentials (B). </li> <li> <p>Record the SCRAM username (A) and SCRAM password (B) to a notepad for reference later. </p> </li> <li> <p>Download the SCRAM certificate (A) and save it to your local machine (B). This will be used later to connect to the Event Streams cluster. </p> </li> </ol>"},{"location":"ea/labs/#produce-data-to-kafka-topic","title":"Produce data to kafka topic","text":"<ol> <li>Export environment variables  <pre><code># looks at bootstrap.servers you created in the previous step\nexport KAFKA_BOOTSTRAP_SERVERS='your.kafka.server:443' \n# use the SCRAM username you generated\nexport KAFKA_USERNAME='your-username' \n# use the SCRAM password you generated\nexport KAFKA_PASSWORD='your-password' \n# use the path to the SCRAM certificate you downloaded. Full path is required.\nexport KAFKA_CA_LOCATION='/path/to/your/cafile.pem'\n</code></pre></li> </ol> <p>\ud83d\udca1 Optional: Data generation with UI  If you want to code with UI, go to JeansGenerator.     Follow the README instructions to run the code. Work in progress.</p>"},{"location":"ea/labs/#setup-python-environment","title":"Setup python environment","text":"<pre><code>python3 -m venv kafkaenv\nsource kafkaenv/bin/activate  # On Windows: kafkaenv\\Scripts\\activate\n````\n\n```bash\n# Install required libraries\npip install -r requirements.txt\n</code></pre> <ol> <li> <p>save this code as <code>kafka_producer.py</code>: <pre><code>import json\nimport uuid\nimport time\nfrom datetime import datetime\nfrom faker import Faker\nfrom confluent_kafka import Producer\nimport random\nimport os\n\n# Setup Faker\nfake = Faker()\n\n# Product attributes\nsizes = [\"XXS\", \"XS\", \"S\", \"M\", \"L\", \"XL\", \"XXL\"]\nmaterials = [\"Classic\", \"Retro\", \"Navy\", \"Stonewashed\", \"Acid-washed\", \"Blue\", \"Black\", \"White\", \"Khaki\", \"Denim\", \"Jeggings\"]\nstyles = [\"Skinny\", \"Bootcut\", \"Flare\", \"Ripped\", \"Capri\", \"Jogger\", \"Crochet\", \"High-waist\", \"Low-rise\", \"Straight-leg\", \"Boyfriend\", \"Mom\", \"Wide-leg\", \"Jorts\", \"Cargo\", \"Tall\"]\ncancel_reasons = [\"BADFIT\", \"TOOEXPENSIVE\", \"CHANGEDMIND\", \"DELAYED\", \"DUPLICATE\"]\n\n\n# Kafka Config with SCRAM using environment variables\nconf = {\n    'bootstrap.servers': os.environ.get('KAFKA_BOOTSTRAP_SERVERS', 'XXX.techzone.ibm.com:443'),\n    'security.protocol': 'SASL_SSL',\n    'sasl.mechanism': 'SCRAM-SHA-512',\n    'sasl.username': os.environ.get('KAFKA_USERNAME', 'XXX'),\n    'sasl.password': os.environ.get('KAFKA_PASSWORD', 'XXX'),\n    'ssl.ca.location': os.environ.get('KAFKA_CA_LOCATION', '/es-producer.pem'),\n}\n\nproducer = Producer(conf)\norders_topic = \"ORDERS\"\ncancellations_topic = \"CANCELS\"\n\ndef generate_order(customerid=None, customer_name=None, quantity=None):\n    size = random.choice(sizes)\n    material = random.choice(materials)\n    style = random.choice(styles)\n\n    return {\n        \"id\": str(uuid.uuid4()),\n        \"customer\": customer_name if customer_name else fake.name(),\n        \"customerid\": customerid if customerid else str(uuid.uuid4()),\n        \"description\": f\"{size} {material} {style} Jeans\",\n        \"price\": round(random.uniform(15.0, 120.0), 2),\n        \"quantity\": quantity if quantity else random.randint(1, 15),\n        \"region\": random.choice([\"EMEA\", \"APAC\", \"AMER\"]),\n        \"ordertime\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n    }\n\ndef generate_cancellation(order):\n    return {\n        \"id\": str(uuid.uuid4()),\n        \"orderid\": order[\"id\"],\n        \"canceltime\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3],\n        \"reason\": random.choice(cancel_reasons)\n    }\n\ndef delivery_report(err, msg):\n    if err is not None:\n        print(f\"\u274c Delivery failed: {err}\")\n    else:\n        print(f\"\u2705 Delivered to {msg.topic()} [{msg.partition()}] offset {msg.offset()}\")\n\ndef send_order(order):\n    producer.produce(orders_topic, key=order[\"customerid\"], value=json.dumps(order), callback=delivery_report)\n    producer.flush()\n\ndef send_cancellation(cancel_msg):\n    producer.produce(cancellations_topic, key=cancel_msg[\"orderid\"], value=json.dumps(cancel_msg), callback=delivery_report)\n    producer.flush()\n\ndef send_split_orders(original_order):\n    num_split = random.randint(2, 5)\n    print(f\"\ud83d\udd01 Generating {num_split} small orders for customer {original_order['customer']} (cancelled large order)\")\n\n    deadline = time.time() + 120  # 2 minutes\n    for i in range(num_split):\n        split_order = generate_order(\n            customerid=original_order[\"customerid\"],\n            customer_name=original_order[\"customer\"],\n            quantity=random.randint(1, 5)\n        )\n        split_order[\"description\"] = original_order[\"description\"]\n        print(f\"\ud83d\udd38 Split order {i+1}: {split_order['description']} | Qty: {split_order['quantity']}\")\n        send_order(split_order)\n\n        if i &lt; num_split - 1:  # Don't wait after last message\n            time_left = deadline - time.time()\n            if time_left &gt; 0:\n                time.sleep(min(time_left / (num_split - i - 1), 20))  # Evenly spread under 2 min\n\nprint(\"\ud83d\udfe2 Kafka producer running (every 10s)\")\n\ntry:\n    while True:\n        order = generate_order()\n        print(f\"\ud83d\udce6 Order: {order['description']} | Qty: {order['quantity']} | Customer: {order['customer']}\")\n        send_order(order)\n\n        if random.random() &lt; 0.5:\n            cancellation = generate_cancellation(order)\n            print(f\"\u274c Cancellation: {cancellation['orderid']} | Reason: {cancellation['reason']}\")\n            send_cancellation(cancellation)\n\n            if order[\"quantity\"] &gt; 10:\n                send_split_orders(order)\n\n        time.sleep(2) # Wait for 2 seconds before sending the next order\n\nexcept KeyboardInterrupt:\n    print(\"\\n\ud83d\uded1 Stopped by user.\")\n</code></pre></p> </li> <li> <p>Run your Python script <pre><code># Make sure you have the required libraries installed\npip install confluent-kafka faker\n# Run the script to produce messages to Kafka\n# Make sure to replace 'XXX' with your actual SCRAM username and password\n# and 'XXX.techzone.ibm.com:443' with your actual Kafka bootstrap server address.\n# Also ensure that the 'es-producer.pem' file is in the same directory as this script.\n# You can run the script using Python 3\npython kafka_producer.py\n</code></pre></p> </li> </ol>"},{"location":"ea/labs/#lab-1-filter-events-based-on-particular-properties","title":"Lab 1 - Filter events based on particular properties","text":"<p>Filter:  When processing events, we can use filter operations to select a subset that we want to use. Filtering works on individual events in the stream.</p> <p>Scenario: Identify orders from a specific region </p> <p>The EMEA operations team wants to move away from reviewing quarterly sales reports and be able to review orders in their region as they occur.</p> <p>Identifying large orders as they occur will help the team identify changes that are needed in sales forecasts much earlier. These results can also be fed back into their manufacturing cycle so they can better respond to demand.</p> <p>Take me to Lab 1</p>"},{"location":"ea/labs/#lab-2-transform-events-to-create-or-remove-properties","title":"Lab 2 - Transform events to create or remove properties","text":"<p>Transform: When processing events we can modify events to remove some properties from the events. Transforms work on individual events in the stream.</p> <p>Scenario: Redact personal information from order events</p> <p>The operations team wants to enable another team to analyze order events, however this other team is not permitted access to personally-identifiable information (PII) about customers. They need to be able to process order events without customer PII.</p> <p>Take me to Lab 2</p>"},{"location":"ea/labs/#lab-3-aggregate-events-to-detect-trends-over-time","title":"Lab 3 - Aggregate events to detect trends over time","text":"<p>Aggregate: Aggregates enable you to process events over a time-window. This enables a summary view of a situation that can be useful to identify overall trends.</p> <p>Transform: When processing events we can modify events to create additional properties, which are derived from the event. Transforms work on individual events in the stream.</p> <p>Scenario: Track how many products of each type are sold per hour</p> <p>In this scenario, we identify the product that has sold the most units in each hourly window. This could be used to drive a constantly updating event streams view of \u201cTrending Products\u201d.</p> <p>Take me to Lab 3</p>"},{"location":"ea/labs/#lab-4-join-related-events-within-time-windows","title":"Lab 4 - Join related events within time windows","text":"<p>Interval join: When looking for patterns in an event stream, sometimes we need to examine events from more than one topic. We talk of this as a \u201cjoin\u201d between the streams - the same term we would use when working with databases and correlating data between two tables.</p> <p>Filter: When processing events we can use filter operations to select a subset that we want to use. Filtering works on individual events in the stream.</p> <p>Scenario: Identify suspicious orders</p> <p>Many interesting situations need us to combine multiple streams of events that correlate events across these inputs to derive a new, interesting situation.</p> <p>In this scenario, we will look for suspicious orders. Specifically, we will be looking for a particular pattern of behavior where large orders have been placed, followed by a smaller order, but the large order was at some point cancelled. This pattern would suggest an attempt to manipulate prices, since the presence of the large order might result in a subsequent reduction in prices, which the smaller order can take advantage of.</p> <p>To find this pattern, we will use the \u201cjoin\u201d capability to compare a stream of \u201corders\u201d with a stream of \u201ccancellations\u201d.</p> <p>Take me to Lab 4</p>"},{"location":"ea/labs/#lab-5-automate-actions-based-on-event-triggers","title":"Lab 5 - Automate actions based on event triggers","text":"<p>Event destination When processing events we can send the results to a new Kafka topic. This lets the results from the flow be used to trigger automations, notifications, business workflows, or be processed further in other applications.</p> <p>Scenario : Distributing results of analysis and processing The EMEA operations team wants to provide a dedicated stream of EMEA order events for further processing.</p> <p>Take me to Lab 5</p>"},{"location":"ea/labs/#lab-6-share-events-for-discovery-by-others","title":"Lab 6 - Share events for discovery by others","text":"<p>Scenario: Sharing results of analysis and processing</p> <p>The EMEA operations team wants to share their new topic of EMEA orders for use by other teams in their enterprise.</p> <p>Take me to Lab 6</p>"}]}